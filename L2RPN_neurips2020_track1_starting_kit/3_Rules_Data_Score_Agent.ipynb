{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./utils/LogoL2RPN.jpg\", width=150, ALIGN=\"left\", border=10>\n",
    "<h1>L2RPN Starting Kit </h1> \n",
    "\n",
    "\n",
    "ALL INFORMATION, SOFTWARE, DOCUMENTATION, AND DATA ARE PROVIDED \"AS-IS\". The CDS, CHALEARN, AND/OR OTHER ORGANIZERS OR CODE AUTHORS DISCLAIM ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR ANY PARTICULAR PURPOSE, AND THE WARRANTY OF NON-INFRIGEMENT OF ANY THIRD PARTY'S INTELLECTUAL PROPERTY RIGHTS. IN NO EVENT SHALL AUTHORS AND ORGANIZERS BE LIABLE FOR ANY SPECIAL, \n",
    "INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF SOFTWARE, DOCUMENTS, MATERIALS, PUBLICATIONS, OR INFORMATION MADE AVAILABLE FOR THE CHALLENGE. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The goal of this challenge is to use Reinforcement Learning approaches for managing a powergrid. The Reinforcement Learning agents will have to automate the control of the powergrid. We use the power network simulator <a href=\"https://github.com/rte-france/Grid2Op\">Grid2Op</a>. It is a simulator that is able to emulate a powergrid of any size and its electrical properties depending on the temporal injections (electricity production and consumption) at each time step.\n",
    "\n",
    "## Goal of this notebook\n",
    "This notebook will briefly describe the how the competition is split. Then it describes the data used, how the participants are ranked (the score and emphisize the difference between the *score* and the *rewards*) and finally conlude by explaining the structure an agent should have.\n",
    "\n",
    "We apologize in advance for its lenght.\n",
    "\n",
    "## References and credits:\n",
    "\n",
    "The creator of Grid2Op was Benjamin Donnot. The competition was designed by Isabelle Guyon, Antoine Marot, Benjamin Donnot and Balthazar Donon. Luca Veyrin, Camillo Romero, Marvin Lerousseau and Kimang Khun are distinguished contributors to the L2RPN challenge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT: Precision on the competition rules\n",
    "\n",
    "***DISCLAIMER*** This part is here to give precision on how the competition will take place and to bring some precision on its rules. Official rules of the competition are distributed in the competition description (on the codalab platform) under the *Terms and Conditions*. Also we remind that this competition is subject to the [general ChaLearn contest rules](http://www.causality.inf.ethz.ch/GeneralChalearnContestRuleTerms.html). It is not possible to use the information given on this notebook to contradict the rules given there. This section is **not part of the official rules**.\n",
    "\n",
    "The competition will take place in three distinct phases, each discribed in one subsection.\n",
    "\n",
    "## The warmup phase\n",
    "This is the first phase of the competition. A warm up phase takes place for each track. During this warmup phase, participants are allowed to make submission on a private dataset (called the *warmup dataset*). This dataset is not included in the training data in any way. It has however the exact same properties: the powergrid has the name number of powerlines, of loads, of generators. They are at the same place, labelled with the same names, have the same physical properties etc.\n",
    "\n",
    "During this phase, participants are encourage to ask question and give feedbacks to the organizers through mails or in our discord server (https://discord.gg/cYsYrPT).\n",
    "If needed, at the end of this phase, organizers can fix the version of grid2op or adapt any of the rules of competition based on community feedback.\n",
    "\n",
    "## The validation phase\n",
    "This represent the real phase of the competition. During this phase, participants will be asked to submit their agents on a different dataset than both the training set and the warmup set. This set will also be private. It has been selected with specific rules from a list of scenarios generated with the same statistical distribution than the training set. We give the guarantee that this method of selection will be the same between the validation phase and the test phase (see below).\n",
    "\n",
    "During this phase (except if some critical bug is discovered or in case of data leakage or in any other critical issue) the version of grid2op used to rank the agent will NOT change. It should be the same one as in the test phase.\n",
    "\n",
    "Participants feedback is still really valuable and we will continue answer to your question on our discord server (https://discord.gg/cYsYrPT) of course. \n",
    "\n",
    "## Test phase\n",
    "Only the score of your agent on this specific phase will be use for the learboard of the competition.\n",
    "\n",
    "**You will not be able to submit anything to this phase**. This phase will last only few hours. The last valid submission on  each participant will be ***automatically*** \"migrated\" to this phase. Only the last agent on this phase will give be used for the final score.\n",
    "\n",
    "\n",
    "## Note on the \"Migration\"\n",
    "Migration is the process with which your agent are automatically ranked in the test phase.\n",
    "\n",
    "This happens automatically, you have nothing to do. You cannot do anything either.\n",
    "\n",
    "Multiple migration can happen for the same participant. For example, if you submit 20 valid agents during the entire validation phase, you will get at least 1 agent migrated and at most 20 agent migrated (exact number depends on the times at which you submit). This entails that some participants will have multiple submission on the final leaderboard displayed on codalab at the end of the competition.\n",
    "\n",
    "We also have a limited number of machines to process your submission. This is why the migration process might take some time. This entails that there might be some delay between the end of the competition and your appearance on the leaderboard of the test phase (this is why the test phase lasts a few hours!). And this also explains why the \"submission date\" in the leaderboard will actually be greater than the last possible dates for which you were able to do submission on the validation phase.\n",
    "\n",
    "For example this was the leaderboard displayed at the end of the WCCI competition:\n",
    "\n",
    "![](./utils/img/LB_example.png)\n",
    "\n",
    "Note how some participants gets multiple submissions. But by design only the ***last one*** will be use to make the final leaderboard.\n",
    "\n",
    "## Final leaderboard\n",
    "\n",
    "Last important note we want to make is the evaluation of the final leaderboard. We included in the rules of the competition some rules that cannot be checked informatically (or that we don't want to check informatically and automatically at each submission). This entails that the final leaderboard on codalab is NOT the final leaderboard of the competition. Final leaderboard of the competition will be updated in our discord and on our website.\n",
    "\n",
    "We want to emphize two importants rules\n",
    "\n",
    "### Double accounts are forbidden\n",
    "We have an explicit rule: 1 team = 1 account.\n",
    "\n",
    "If we notice that the participants of the same team appears more than once in the final leaderboard, we remove all associated entries. We won't keep the best one, we won't pick one at random, we will simply removed all double \"double submissions\" from this competition.\n",
    "\n",
    "If you want to compete as a team, the best way is to create a common mail adress, one mail address all team members will have access to.\n",
    "\n",
    "We allow participants to form team during the \"validation phase\" of the competition and only at least **2 weeks before the beginning of the test phase**. If that is the case, please send the organiser an email (or a message on discord) with the name (on codalab) and mail addresses of each team member. We will proceed to the erase of all team members submission in the test set to avoid duplicate at that time. \n",
    "\n",
    "Note that in this case, if any team members submit another agent with his account after the team has been created and validated by one of the organizer, it will break the \"double accounts are forbidden\" rules this is thus forbidden (errors can happen, if you realise you make a mistake come forward and we'll fix it for you for sure).\n",
    "\n",
    "### Submission are limited in size\n",
    "\n",
    "We don't check the size of your model in the codalab platform. Know that submissions are limited to 500MB in size (real size might vary and this notebook will not be necessarily updated, the real size is written in the ***Terms and Conditions*** page of the official competition). This means that if the zip file you send to codalab is more than 500MB, it will be removed from the leadeboard.\n",
    "\n",
    "### Prices and open sourcing of submission\n",
    "We recall that in section *8 Prizes and Awards* section *(a) Prizes* of the [general ChaLearn contest rules](http://www.causality.inf.ethz.ch/GeneralChalearnContestRuleTerms.html) stipulates that, in order to receive your price, you ***must*** open source your code (this is also reminded in the  ***Terms and Conditions***) [*disclaimer: this wording might not be legally correct in all countries and in all domains. We recall that this paragraph is NOT part of the rules. Please refer to the header ot this section for more information.*].\n",
    "\n",
    "For this precise competition this means that you should, in order to receive your price (in case you are elligible to such price), follow the template baseline of the [l2rpn-baselines](https://github.com/rte-france/l2rpn-baselines) github repository available at this page [CONTRIBUTE](https://github.com/rte-france/l2rpn-baselines/blob/master/CONTRIBUTE.md).\n",
    "\n",
    "What does it mean? It means that anyone that wants it (under the licensing of your chosing, see more information on the chalearn contest rules and on the l2rpn-baselines github) should be able to run your *trained agent* on the very same competition. Your trained agent will also be somehow included in the l2rpn-baselines python package.\n",
    "\n",
    "What it does not mean? It does not mean that your *training code*. Of course, this is a competition toward an open science project aiming as at much transparency as possible. But we understand that for some person (academics who's paper are under review, or industrial method) it is not possible to ask to disclose the training methodology. This is the main reason why we allow you not to share the code of the training of your agent in case you are eligible to some prices.\n",
    "\n",
    "**NB** These competitions aim at closing the gap between industrial real time grid operation and planning and the research community in \"Sequential Decision Making\" to help develop further new kind of industrial or academic partnership.\n",
    "\n",
    "RTE (main organizer), nor any of the competition sponsors, will NEVER use your algorithm without your consent. Your code is subject to the international laws on copy right and (if applicable) to the license (open source or not) you released it under. We will never disclose your code to anyone (except to a group of experts from RTE and other relevant affiliation to look at it to make sure it is compliant with the rules of the competition). \n",
    "\n",
    "\n",
    "# Important note on the data\n",
    "\n",
    "Power system are rather complex system. It is not easy to generate \"realistic\" data in an automatic manner. To make this competition happen we had two choices:\n",
    "\n",
    "- Providing each participant a data generator so that each of you can generate as much data as you want, but without being sure these datasets were realistic\n",
    "- Providing each participants with a limited number of data (that we called scenarios) that we carefully checked in order to make sure they were \"realistic\" or rather \"as realistic as the data your agents will be tested on\".\n",
    "\n",
    "We decided to go for the second option for multiple reasons. The first reason is practical: it is to reduce the entry cost in these competitions. Indeed having to use an external tool to generate the data would have been a barrier. Typing a command like `grid2op.make(xxx)` is by far easier. The main reason also is that the generation process can be quite complex and it is easy to have irrealistic datasets. \n",
    "\n",
    "Your agent can be trained on a fix number of *scenarios* that we provide (we call that the *training set*), this is the data you will retrieve with the command in the next section. And will be tested on a fix number of scenarios that we kept hidden (one set of scenarios for the *warm up phase* one different set of scenarios for the *validation phase* and one for the *test phase*).\n",
    "\n",
    "You can make the parrallel between what we call a *scenario* and the concept of *level* in certain video game, Prince of Persia, Rayman, Mario etc. In that case, for each track, your agent will play always the same game (say [Super_Mario_Bros](https://en.wikipedia.org/wiki/Super_Mario#Super_Mario_Bros.)). Your agent can train on a fix set of levels (the *training levels*) and will be tested on different (by similar and coming from the same statistical distribution) levels on our servers (when you submit your agent on codalab, see next notebooks).\n",
    "\n",
    "For each of the neurips track we provide two different kind of *training datasets*. One \"*small*\" and one \"*large*\" dataset. The small one is a subset of the first element of the large one. We don't recommend you to use the \"large\" dataset at first, why ?\n",
    "\n",
    "### Track 1\n",
    "For this track 1, the \"*small*\" dataset contains 48 years of data. Each years counts 12 months (*yes yes even in power system the years counts 12 months ;-)*) and each month have 4 weeks (*wait what ??? Yes for this competition month of March will have 28 days. I told you in power system we didn't like to do things like everyone else*) each of 7 days counting exactly 24 hours (no time change in these simulated dataset). We decided that a \"step\" reprenseted 5 mins for this competition.\n",
    "\n",
    "This means that for the *small* dataset you have at your disposal `48 years * 12 months * 4 weeks * 7 days * 24 hours * (60/5) steps = 4.644.864` different input data. This is a pretty large number of powergrid states, trust me. If predicting an action with a neural network takes like 10 ms (which would be a super fast model) you will need (without training!, just to predict the best action in all these states) approximately 13 consecutive hours. This dataset weights 800~900MB\n",
    "\n",
    "The large dataset is made of 240 years, representing in total `240 * 12 * 4 * 7 * 24 * (60/5) = 23.224.320` different input data. This is really large and we don't recommend you to download this *large* dataset unless you feel pretty confident that your model is lacking training data. This dataset weights 4.5-5GB.\n",
    "\n",
    "**NB** actually, the *scenarios* are not deterministic, they are stochastics (due to maintenance) and adversarial (with the opponent). In our opinion, even using the *small* dataset is enough for the vast majority of models to perform well. \n",
    "\n",
    "### Track 2\n",
    "For this track 2 the data are a bit different. Indeed, instead of studying one type of scenarios (*ie* instead of having one single distribution that generates the loads and generations at each element of the grid) there are many (as we explained on the previous notebook). For the \"track2\" dataset you have grids coming with different energy mixes: some having more renewable energy than the others.\n",
    "\n",
    "On this track, for each dataset you have 5 mixes (\"x1\", \"x1.5\", \"x2\", \"x2.5\" and \"x3\"). And for each of these 5 environments, you have:\n",
    "- for the \"*small*\" dataset 10 years making, for each mixes of the \"small\" dataset `10 years * 12 months * 4 weeks * 7 days * 24 hours * (60/5) steps = 967.680` steps. With 5 mixes in total: `5 mixes * 967.680 steps / mixes = 4.838.400` that weights 2~2.5GB\n",
    "- for the \"*large*\" dataset 48 years are present for each mixes weighting in total 10-11GB.\n",
    "\n",
    "**NB** actually, the *scenarios* are not deterministic, they are stochastics (due to maintenance) and adversarial (with the opponent). In our opinion, even using the *small* dataset is enough for the vast majority of models to perform well. \n",
    "\n",
    "# 1 - Loading the environment\n",
    "\n",
    "The first time that you build the environment, grid2op will automatically download all the corresponding data (about 4-5Go for the smallest dataset).\n",
    "\n",
    "We will first create an agent and (re) explain some of the basics of grid2op."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import os\n",
    "#%matplotlib inline\n",
    "# Uncomment the next lines to auto-reload libraries (this causes some problem with pickles in Python 3)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#import seaborn as sns; sns.set()\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import evaluate\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "import datetime \n",
    "import zipfile\n",
    "from grid2op import make\n",
    "import os\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from utils import problem_dir, score_dir, input_data_check_dir, output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track = \"track1\"\n",
    "# track = \"track2\"\n",
    "if not 'track' in globals():\n",
    "    raise RuntimeError(\"Please specify if you want to compete on track1 (opponent) or track2 (different mix) \"\\\n",
    "                       \"by uncommenting one of the line above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB** The environment for the track1 and track2 of the neurips competition are different. This notebook is the same for the both track, though the dataset are different, this is why we advise you to tell on which track you want to compete :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a path for our submission.\n",
    "\n",
    "In this file we use the standard submission in the \"example_submissions\" make sure to have a look at them. They all explain how to make a submission :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'example_submissions/submission'\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the zipped file that you will upload to Codalab can be named how you want, but **the submission folder (for this notebook) absolutely must be named \"submission\", as we did here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if track == \"track1\":\n",
    "    env = make(\"l2rpn_neurips_2020_track1_small\")\n",
    "elif track == \"track2\":\n",
    "    env = make(\"l2rpn_neurips_2020_track2_small\")\n",
    "else:\n",
    "    raise RuntimeError(\"This notebook is the starting kit of the l2rpn Neurips 2020 competition. You can only \"\\\n",
    "                      \"use it with track=\\\"track1\\\" or track=\\\"track2\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - The Reward and the Score of an agent\n",
    "\n",
    "### Reward\n",
    "The Reward is the quantity that your agent will aim to maximize. This is a rather personal choice, you can choose any reward function that you think is adequate.\n",
    "\n",
    "Grid2op allows for a large variety of such reward functions. You can visit [rewards in grid2op](https://grid2op.readthedocs.io/en/latest/reward.html) for more information about rewards.\n",
    "\n",
    "In this competition, you can use any of the provided reward functions, or your own, to train your agent and to assess its performance. To do that, you need to have the reward class that you want to use as a `reward` variable in your `__init__.py`. That class must be a subclass of `grid2op.Reward.BaseReward.BaseReward`. See the official help for the competition on our discord https://discord.gg/cYsYrPT if you need some help. \n",
    "\n",
    "More examples will be provided for the complete competition.\n",
    "\n",
    "### Score\n",
    "The Score is the quantity that is used to compare your agent with the agents of the other participants.\n",
    "\n",
    "To begin with, we will recall that transporting electricity always generates some energy losses $\\mathcal{E}_{loss}(t)$ due to the Joule effect in resistive powerlines at any time $t$:\n",
    "\\begin{equation}\n",
    "    \\mathcal{E}_{loss}(t)=\\sum\\limits_{l=1}^{n_{l}}r_l*{y_l}(t)^2\n",
    "\\end{equation}\n",
    "Where $r_l$ is the resistance of powerline $l$ and $y_l$ is the current flowing through it.\n",
    "\n",
    "At any time $t$, the operator of the grid is responsible for compensating those energy losses by purchasing on the energy market the corresponding amount of production at the marginal price $p(t)$. We can therefore define the following energy loss cost $c_{loss}(t)$:\n",
    "\n",
    "\\begin{equation}\n",
    "c_{loss}(t)=\\mathcal{E}_{loss}(t)*p(t)\n",
    "\\end{equation}\n",
    "\n",
    "Then we should consider that the decisions made by the operator can induce costs, especially when they require market players to perform specific actions, as they should be paid in return. Topological actions (modifying the structure of the grid) are mostly free, since the grid belongs to the powergrid operator, and no energy cost is involved. However, energy producers are affected by the redispatching actions (having some generators produce more energy and others produce less) and should get paid. As the grid operators ask to redispatch some energy $\\mathcal{E}_{redispatch}(t)$, some power plants will increase their production by $\\mathcal{E}_{redispatch}(t)$ while others will compensate by decreasing their production by the same amount to keep the power grid balanced. Hence, the grid operator will pay both producers for this redispatched energy at an additional cost $c_{redispatching(t)}$, higher than the marginal price $p(t)$ by some factor $\\alpha$:\n",
    "\n",
    "\\begin{equation}\n",
    "c_{redispatching}(t)=2*\\mathcal{E}_{redispatch}*\\alpha p(t),\\ \\alpha \\geqslant1\n",
    "\\end{equation}\n",
    "\n",
    "Indeed, the first producer has to be paid an extra $\\mathcal{E}_{redispatch}*\\alpha p(t)$ because he has to produce $\\mathcal{E}_{redispatch}$ more energy than it had planned to, and the second producer also has to be paid an extra $\\mathcal{E}_{redispatch}*\\alpha p(t)$ to compensate for the $\\mathcal{E}_{redispatch}$ energy that it did not produce and sell.\n",
    "\n",
    "If no flexibility is identified or integrated into the grid, operational costs related to redispatching can dramatically increase due to renewable energy sources (since the production from these energy sources can vary significantly throughout a year) as was the case recently in Germany with **an avoidable 1 billion €/year increase**.\n",
    "\n",
    "Hence, we can define our overall operational cost $c_{\\text{operations}}(t)$:\n",
    "\\begin{equation}\n",
    "c_{\\text{operations}}(t)=c_{\\text{loss}}(t)+c_{\\text{redispatching}}(t)\n",
    "\\end{equation}\n",
    "\n",
    "Formally, we can define an \"episode\" $e$ successfully managed by an agent up to a time $t_{\\text{end}}$ (on a scenario of maximum length $T_e$) by:\n",
    "\\begin{equation}\n",
    "e = \\left(o_1, a_1, o_2, a_2,\\dots, a_{t_{\\text{end}}-1}, o_{t_{\\text{end}} }\\right)\n",
    "\\end{equation}\n",
    "where $o_t$ represents the observation at time $t$, and $a_t$ the action that the agent took at time t. In particular, $o_1$ is the first observation and $o_{t_{\\text{end}}}$ is the last one. The scenario ended at time $t_{end}$, either because there was a game over or because the agent reached the end of the scenario.\n",
    "An agent can either manage to operate the grid for the entire scenario ($t_{\\text{end}} = T_e$) or fail after some time $t_{\\text{end}}$ because of a blackout. In case of a blackout, the cost $c_{\\text{blackout}}(t)$ at a given time t would be proportional to the amount of consumption that was not supplied, $\\text{Load}(t)$, at a price higher than the marginal price $p(t)$ by some factor $\\beta$:\n",
    "\\begin{equation}\n",
    "c_{\\text{blackout}}(t)=\\text{Load}(t)*\\beta*p(t), \\ \\beta \\geqslant1\n",
    "\\end{equation}\n",
    "Notice that $\\text{Load}(t) >> \\mathcal{E}_{\\text{redispatch}}(t)$ or $\\mathcal{E}_{\\text{loss}}(t)$\n",
    "which means that the cost of a blackout is a lot higher than the cost of operating the grid as expected. It is even higher if we further consider the secondary effects on the economy. More information can be found thanks to <a href=\"https://www.blackout-simulator.com/\">this blackout cost simulator</a>. Furthermore, a blackout does not last forever and power grids restart at some point, but for the sake of simplicity while preserving most of the realism, all these additional complexities are not considered here so the scenario will be terminated in case of a game over.\n",
    "\n",
    "Now we can define our cost $c$ for an episode:\n",
    "\\begin{equation}\n",
    "c(e)=\\sum\\limits_{t=1}^{t_{\\text{end}}} c_{\\text{operations}}(t) + \\sum\\limits_{t=t_{\\text{end}}}^{T_{e}}c_{\\text{blackout}}(t)\n",
    "\\end{equation}\n",
    "\n",
    "The participants are encouraged to operate the grid for as long as possible, and will be penalized for a blackout even after the game is over, until $T_e$, as this is a critical system and safety is paramount.\n",
    "\n",
    "Finally, participants will be tested on $N$ hidden scenarios of different lengths, varying from one day to one week, and on various situations that proved difficult to our baselines. This will be the way to test the agent's behavior in various representative conditions. The overall score to minimize over all the scenarios will be :\n",
    "\n",
    "\\begin{equation}\n",
    "Score=\\sum\\limits_{i=1}^{N}c(e_i)\n",
    "\\end{equation}\n",
    "\n",
    "### Rescaling the scores\n",
    "For the `DoNothing` agent this score was really high on our scenarios, around 33 billions. Since this is less readable, we decided to apply a linear transformation such that:\n",
    "- the score is 100 for the best possible agent (an agent that handles all the scenarios, without using redispatching actions, with minimal losses of $1%$ for all the scenarios)\n",
    "- the score is 0 for the `DoNothing` agent\n",
    "\n",
    "This means that:\n",
    "- the score should be **maximized** rather than minimized\n",
    "- having a score of 100 is probably out of reach\n",
    "- having a positive score is already pretty good and means that your agent is better than the `DoNothing` agent\n",
    "\n",
    "### But which reward should I use ?\n",
    "As we said earlier, the reward is a personnal choice. If you have two exact same type of agent (same training scheme, same neural network model, same everything...) and use two different reward it is likely that one will perform better on this competition thatn the other (though they will be evaluated with the same `score`).\n",
    "\n",
    "This is common on machine learning. For example, in supervised machine learning you can find similar behaviour even for some problems as common as image classification on [ImageNet](http://www.image-net.org/) for example. Indeed for this competition most neural networks are trained with the [categorical cross entropy loss](https://en.wikipedia.org/wiki/Cross_entropy) while for the \"ImageNet competition\" (named *Large Scale Visual Recognition Challenge*) the ranking used the top1 or top5 accuracy. Choosing a different training loss can yield to dramatic performance changes.\n",
    "\n",
    "If you make the parrallel between the \"reward\" you use for training and the \"training loss\" you have the same flexibility in this RL competition. You can change the \"reward\" (just as you can change the training loss when you train a algorithm to label images in a supervised way).\n",
    "\n",
    "That being said, to reach the highest scores:\n",
    "\n",
    "1. the highest the number of scenarios your agent perform before a game over, the higher your score will be. We can advise you to use a reward that meet this property (one way is to have always positive reward, so that the cumulative reward will always go up if your agent survives longer). But again that is only an advise. Sometimes penalizing your agent when he really takes bad decision is a terrible idea (in that case negative reward can be a good idea). The number of time steps your agent can actually manage without game over is the most important thing to increase your score.\n",
    "- the lowest the amount of redispatching, the highest the score. Indeed, from a TSO (company handling the powergrid) point of view redispatching can be thought at compensating the electricity producers because the grid cannot handle their productions. The lowest the *redispatching*, the lowest the compensation, the highest the score. That being said, sometimes it is mandatory to do *redispatching* because of that is the only way not to *game over*. In that case doing redispatching is necessary to play a few more time steps and drastically increase your score. \n",
    "- in third comes the losses. Due to [Joule's effect](https://en.wikipedia.org/wiki/Joule_effect) most TSO have somehow to compensate the losses on the grid. In that case, the lower the losses, the better. Including in your agent a component of the reward that includes how well you are performing on the losses is probably a good idea.\n",
    "\n",
    "A list of possible rewards is given in the grid2op [rewards documentation](https://grid2op.readthedocs.io/en/latest/reward.html). \n",
    "* The closest reward to the score is the [RedispReward](https://grid2op.readthedocs.io/en/latest/reward.html#grid2op.Reward.RedispReward) it takes mainly into account the points 2. and 3. above. \n",
    "* The [FlatReward](https://grid2op.readthedocs.io/en/latest/reward.html#grid2op.Reward.FlatReward) just give a reward of 1 depending on the number of time steps your agent performed and might a suitable option (strongly emphasize the first point above). \n",
    "* The [L2RPNReward](https://grid2op.readthedocs.io/en/latest/reward.html#grid2op.Reward.L2RPNReward) gives more point if the flows on each powerlines is \"even\" (higher score if all powerlines are 50% loaded compare to a situation with 2 or 3 powerlines loaded at 80% and the rest at 40% for example) this is a relative straightforward (though imperfect) criteria that stronly correlate with point 1. above.\n",
    "* The [CombinedScaledReward](https://grid2op.readthedocs.io/en/latest/reward.html#grid2op.Reward.CombinedScaledReward) allows you to... Combine multiple reward (do their sum) and scale them (to emphasize more this or that).\n",
    "\n",
    "***TL;DR (Too Long; Didn't Read)***: reward is a personnal choice for lots of good reasons. Chosing a good reward that allows your agent to learn a safe policy is a crucial part of this challenge. You have the full liberty to define the reward you want :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Building an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide simple examples of agents in the `starting-kit/example_submissions` directory. We will show an example here using the most simple agent: the \"do nothing\" agent, that never modifies anything. To make your own agent, you should create a subclass of the `grid2op.Agent.BaseAgent.BaseAgent` class and implement your own act method as shown below.\n",
    "    \n",
    "**NB** For the real competition, a repository containing several baseline agents will be open source. We are actively working on it but are currently facing some open source license issues for this sandbox competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a submission on the challenge, **you must create a folder containing a `__init__.py` script in which a `make_agent` function (and optionally a `reward` variable as we will see later) will have to be defined. That function should return an instance of your agent.**\n",
    "\n",
    "The `make_agent` function takes two argument, `env` and `submission_dir` and must return an instance of your agent.\n",
    "\n",
    "That instance (your agent) must implement an `act` method, which has to take the arguments `observation`, `reward` and `done` and must return the chosen action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your final submission folder **must** look like:\n",
    "```bash\n",
    "submission\n",
    "├── some_other_script.py\n",
    "├── __init__.py\n",
    "```\n",
    "\n",
    "In particular, **the folder must be named \"submission\"**.\n",
    "The folder can also include other directories and any data that your scripts need.\n",
    "\n",
    "It is also possible to use symbolic links that point to files or folders elsewhere. These files or folders will be copied to replace the symbolic links when the folder is zipped. This allows you to work in your development directory and simply add symbolic links in your submission directory that point to your files or folders where you work. This way, you do not have to copy/paste them to your submission directory, that will be done automatically when the folder is zipped.\n",
    "\n",
    "Once that folder is zipped, it will look like:\n",
    "```bash\n",
    "any_name.zip\n",
    "├── submission\n",
    "│   ├── some_other_script.py\n",
    "│   ├── __init__.py\n",
    "├── metadata\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example. Let's create three scripts in our folder : `__init__.py`, `submission.py` in which we will define the needed variables for the submission, and `agents.py` in which we will define our agent.\n",
    "We will name the folder `submission`, which is necessary as we discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agents** : This is the `my_agent.py` script where we define our agent class as a subclass of `grid2op.Agent.BaseAgent.BaseAgent`. It implements the `act` method. Here is an example of agent that does nothing (it is equivalent to the `DoNothing` agent) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile example_submissions/submission/my_agent.py\n",
    "\n",
    "from grid2op.Agent import BaseAgent\n",
    "\n",
    "class MyAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    The template to be used to create an agent: any controller of the power grid is expected to be a subclass of this\n",
    "    grid2op.Agent.BaseAgent.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        \"\"\"Initialize a new agent.\"\"\"\n",
    "        BaseAgent.__init__(self, action_space=action_space)\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\"The action that your agent will choose depending on the observation, the reward, and whether the state is terminal\"\"\"\n",
    "        # do nothing for example (with the empty dictionary) :\n",
    "        return self.action_space({})\n",
    "    \n",
    "def make_agent(env, this_directory_path):\n",
    "    my_agent = MyAgent(env.action_space)\n",
    "    return my_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`__init.py__`** : This is the script that will be read by Codalab. Here we simply load the required variables for the submission :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile example_submissions/submission/__init__.py\n",
    "\n",
    "from .my_agent import make_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our folder is correctly set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines\n",
    "\n",
    "We are actively working on some baselines that you will be able to reuse easily.\n",
    "\n",
    "You have already an example of such in the `example_submissions/submission_withbaselines` directory of this starting kit. This baselines comes from the [l2rpn-baselines](https://github.com/rte-france/l2rpn-baselines) github repostory / ptyhon package with available online help [here](https://l2rpn-baselines.readthedocs.io/en/latest/).\n",
    "\n",
    "For more information, you can also visit our discord server at https://discord.gg/cYsYrPT !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<h1> 4 - Making a submission </h1> \n",
    "\n",
    "We will see in the next notebook how to submit our agent to Codalab.\n",
    "    \n",
    "<h1> 5 - Other examples </h1> \n",
    "\n",
    "Make sure to check the folder \"example_submission\" included in this starting kit that contains some example on valid submission (these submissions are not here to demonstrate how you can solve the problem, but are rather here to show how to submit a valid file on codalab, which is not always easy)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
